{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install torch_geometric"
      ],
      "metadata": {
        "id": "JbdngmepFgVw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import preprocessing as pp\n",
        "from torch_geometric.data import HeteroData, download_url, extract_zip\n",
        "from keras.layers import Dropout, Flatten, Activation, Input, Embedding, BatchNormalization, Dense, dot\n",
        "from keras.optimizers import Adam\n",
        "from pylab import rcParams\n",
        "from tqdm.notebook import tqdm\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import keras\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import scipy.sparse as sp\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import random"
      ],
      "metadata": {
        "id": "6MLX4hX8f8bn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def download(url, root=os.getcwd()) -> None:\n",
        "    # ref: https://pytorch-geometric.readthedocs.io/en/stable/_modules/torch_geometric/datasets/movie_lens_100k.html#MovieLens100K\n",
        "    path = download_url(url, root)\n",
        "    extract_zip(path, root)\n",
        "    os.remove(path)\n",
        "\n",
        "    folder_name = url.split(\"/\")[-1].split(\".\")[0]\n",
        "    # folder = os.path.join(root, folder_name)\n",
        "    # fs.rm(raw_dir)\n",
        "    # os.rename(folder, raw_dir)\n",
        "    return os.path.join(root, folder_name)\n",
        "\n",
        "url = \"https://files.grouplens.org/datasets/movielens/ml-100k.zip\"\n",
        "\n",
        "raw_file_names = [\n",
        "    \"u.item\",\n",
        "    \"u.user\",\n",
        "    \"u.data\",\n",
        "]\n",
        "# ['u.item', 'u.user', 'u1.base', 'u1.test']"
      ],
      "metadata": {
        "id": "CAdr4osSFLnj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "USER_HEADERS = [\"user_id\", \"age\", \"gender\", \"occupation\", \"zip_code\"]\n",
        "MOVIE_HEADERS = [\n",
        "    \"item_id\",\n",
        "    \"title\",\n",
        "    \"release_date\",\n",
        "    \"video_release_date\",\n",
        "    \"IMDb URL\",\n",
        "    \"unknown\",\n",
        "    \"Action\",\n",
        "    \"Adventure\",\n",
        "    \"Animation\",\n",
        "    \"Children's\",\n",
        "    \"Comedy\",\n",
        "    \"Crime\",\n",
        "    \"Documentary\",\n",
        "    \"Drama\",\n",
        "    \"Fantasy\",\n",
        "    \"Film-Noir\",\n",
        "    \"Horror\",\n",
        "    \"Musical\",\n",
        "    \"Mystery\",\n",
        "    \"Romance\",\n",
        "    \"Sci-Fi\",\n",
        "    \"Thriller\",\n",
        "    \"War\",\n",
        "    \"Western\",\n",
        "]\n",
        "RATING_HEADERS = [\"user_id\", \"item_id\", \"rating\", \"timestamp\"]\n",
        "\n",
        "\n",
        "folder_path = download(url)\n",
        "raw_paths = [os.path.join(folder_path, i) for i in raw_file_names]"
      ],
      "metadata": {
        "id": "VY6qyyMeE7WG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2CjsFd_laV5P"
      },
      "outputs": [],
      "source": [
        "user_df = pd.read_csv(\n",
        "    raw_paths[1],\n",
        "    sep=\"|\",\n",
        "    header=None,\n",
        "    names=USER_HEADERS,\n",
        "    # index_col='user_id',\n",
        "    encoding=\"ISO-8859-1\",\n",
        ")\n",
        "\n",
        "item_df = pd.read_csv(\n",
        "    raw_paths[0],\n",
        "    sep=\"|\",\n",
        "    header=None,\n",
        "    names=MOVIE_HEADERS,\n",
        "    # index_col='item_id',\n",
        "    encoding=\"ISO-8859-1\",\n",
        ")\n",
        "\n",
        "df = pd.read_csv(\n",
        "    raw_paths[2],\n",
        "    sep=\"\\t\",\n",
        "    header=None,\n",
        "    names=RATING_HEADERS,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "nHmKcLvwYwS3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "user_df"
      ],
      "metadata": {
        "id": "qC3DdQj6Y0Na"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "item_df"
      ],
      "metadata": {
        "id": "-bfyGC8XY1ri"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5 Types of CF Models: Matrix Factorization (MF), Bayesian Personalized Ranking (BPR), Neural Collaborative Filtering (NCF), Convolutional Matrix Factorization (ConvMF), Factorization Machine (FM)"
      ],
      "metadata": {
        "id": "z4cB6ns0q8Gx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Low Rank Matrix Factorization (MF + NCF)"
      ],
      "metadata": {
        "id": "SYZkkuljZvFy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Understanding"
      ],
      "metadata": {
        "id": "NB8yMoBZc3xu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['user_id'].unique()"
      ],
      "metadata": {
        "id": "k_TkXPnAZpuu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(df['user_id'].unique())"
      ],
      "metadata": {
        "id": "7APXX465Z-ww"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['item_id'].unique()"
      ],
      "metadata": {
        "id": "lDoPgAdcaJGW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(df['item_id'].unique())"
      ],
      "metadata": {
        "id": "jLp6JGxSaQld"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['user_id'].isnull().sum()"
      ],
      "metadata": {
        "id": "YtWR4vWwcl4-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['rating'].isnull().sum()"
      ],
      "metadata": {
        "id": "cjHN53T6cqkG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['item_id'].isnull().sum()"
      ],
      "metadata": {
        "id": "jZgZ53DKcrUk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preparation"
      ],
      "metadata": {
        "id": "rjn9cKeXc6kd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Encoding Columns"
      ],
      "metadata": {
        "id": "FiJVSZjNdvsI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.user_id = df.user_id.astype('category').cat.codes.values\n",
        "df.item_id = df.item_id.astype('category').cat.codes.values"
      ],
      "metadata": {
        "id": "pjzRGpJwc8P-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['user_id'].value_counts(ascending=True)"
      ],
      "metadata": {
        "id": "z85oQsI3dVs-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['item_id'].unique()"
      ],
      "metadata": {
        "id": "3GZNVVJGdXZL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create Utility Matrix"
      ],
      "metadata": {
        "id": "KNdW0aURdy4R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# creating utility matrix.\n",
        "index=list(df['user_id'].unique())\n",
        "columns=list(df['item_id'].unique())\n",
        "index=sorted(index)\n",
        "columns=sorted(columns)\n",
        "\n",
        "util_df=pd.pivot_table(data=df,values='rating',index='user_id',columns='item_id')\n",
        "# Nan implies that user has not rated the corressponding movie."
      ],
      "metadata": {
        "id": "aueR-m0qdt88"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "util_df"
      ],
      "metadata": {
        "id": "7jhuIvVJd8Ef"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "util_df.fillna(0)"
      ],
      "metadata": {
        "id": "SojNtOs7eABZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating Training and Validation Sets"
      ],
      "metadata": {
        "id": "UUzbjSiMfI2u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# x_train,x_test,y_train,y_test=train_test_split(df[['userId','movieId']],df[['rating']],test_size=0.20,random_state=42)\n",
        "users = df.user_id.unique()\n",
        "items = df.item_id.unique()\n",
        "\n",
        "userid2idx = {o:i for i,o in enumerate(users)}\n",
        "itemsid2idx = {o:i for i,o in enumerate(items)}"
      ],
      "metadata": {
        "id": "5KDdDyOkfP97"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['user_id'] = df['user_id'].apply(lambda x: userid2idx[x])\n",
        "df['item_id'] = df['item_id'].apply(lambda x: itemsid2idx[x])\n",
        "split = np.random.rand(len(df)) < 0.8\n",
        "train = df[split]\n",
        "valid = df[~split]\n",
        "print(train.shape , valid.shape)"
      ],
      "metadata": {
        "id": "5xMfTDNbfOZ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Matrix Factorization"
      ],
      "metadata": {
        "id": "obCUW3HYfk7P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating the Embeddings ,Merging and Making the Model from Embeddings"
      ],
      "metadata": {
        "id": "aD-XZwvlfoYG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_items=len(df['item_id'].unique())\n",
        "n_users=len(df['user_id'].unique())\n",
        "n_latent_factors=64  # hyperparamter to deal with."
      ],
      "metadata": {
        "id": "2Z_rX8kGfmgX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "user_input=Input(shape=(1,),name='user_input',dtype='int64')"
      ],
      "metadata": {
        "id": "QzPVcdB0fuNS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "user_embedding=Embedding(n_users,n_latent_factors,name='user_embedding')(user_input)\n",
        "user_embedding.shape"
      ],
      "metadata": {
        "id": "yrOCN-aXgJam"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "user_vec =Flatten(name='FlattenUsers')(user_embedding)\n",
        "user_vec.shape"
      ],
      "metadata": {
        "id": "Msq4kVRqgTh-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "item_input=Input(shape=(1,),name='item_input',dtype='int64')\n",
        "item_embedding=Embedding(n_items,n_latent_factors,name='item_embedding')(item_input)\n",
        "item_vec=Flatten(name='FlattenMovies')(item_embedding)\n",
        "item_vec"
      ],
      "metadata": {
        "id": "Azg0pssIgU0R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sim=dot([user_vec,item_vec],name='Simalarity-Dot-Product',axes=1)\n",
        "model =keras.models.Model([user_input, item_input],sim)\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "EfpKaUd0gh7O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Compile Model"
      ],
      "metadata": {
        "id": "8FqZps1LiVYU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer=Adam(learning_rate=1e-4), loss='mse')"
      ],
      "metadata": {
        "id": "pCV3INR7iWdx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train.shape\n",
        "batch_size=128\n",
        "epochs=50"
      ],
      "metadata": {
        "id": "JCBxm2q3iwF5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fitting on Training set & Validating on Validation Set"
      ],
      "metadata": {
        "id": "qKNnIhwLi1vB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "History = model.fit([train.user_id,train.item_id],train.rating, batch_size=batch_size,\n",
        "                              epochs =epochs, validation_data = ([valid.user_id,valid.item_id],valid.rating),\n",
        "                              verbose = 1)"
      ],
      "metadata": {
        "id": "AlycuV4ii29c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluating Model Performance"
      ],
      "metadata": {
        "id": "zq2MB-qei-nS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rcParams['figure.figsize'] = 10, 5\n",
        "\n",
        "plt.plot(History.history['loss'] , 'g')\n",
        "plt.plot(History.history['val_loss'] , 'b')\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "HkgR68ecjBge"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Neural Network"
      ],
      "metadata": {
        "id": "WFgWb3uZkbFY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating Embeddings"
      ],
      "metadata": {
        "id": "uOIvKKP6k7ml"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_latent_factors=50\n",
        "n_items=len(df['item_id'].unique())\n",
        "n_users=len(df['user_id'].unique())"
      ],
      "metadata": {
        "id": "jOjhLUWgkc7e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "user_input=Input(shape=(1,),name='user_input',dtype='int64')\n",
        "user_embedding=Embedding(n_users,n_latent_factors,name='user_embedding')(user_input)\n",
        "user_vec=Flatten(name='FlattenUsers')(user_embedding)\n",
        "user_vec=Dropout(0.40)(user_vec)"
      ],
      "metadata": {
        "id": "PkcLfdo4kjos"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "item_input=Input(shape=(1,),name='item_input',dtype='int64')\n",
        "item_embedding=Embedding(n_items,n_latent_factors,name='item_embedding')(item_input)\n",
        "item_vec=Flatten(name='FlattenItems')(item_embedding)\n",
        "item_vec=Dropout(0.40)(item_vec)"
      ],
      "metadata": {
        "id": "Q3g540iNklmj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sim=dot([user_vec,item_vec],name='Simalarity-Dot-Product',axes=1)"
      ],
      "metadata": {
        "id": "mBtltsoXk2iw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Specifying Model Architecture"
      ],
      "metadata": {
        "id": "tySmAaQqk9rC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nn_inp=Dense(96,activation='relu')(sim)\n",
        "nn_inp=Dropout(0.4)(nn_inp)\n",
        "nn_inp=BatchNormalization()(nn_inp)\n",
        "nn_inp=Dense(1,activation='relu')(nn_inp)\n",
        "nn_model =keras.models.Model([user_input, item_input],nn_inp)\n",
        "nn_model.summary()\n"
      ],
      "metadata": {
        "id": "1LMWNsovlA1z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Compiling Model"
      ],
      "metadata": {
        "id": "HVvgcqLblVPW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nn_model.compile(optimizer=Adam(learning_rate=1e-3),loss='mse')"
      ],
      "metadata": {
        "id": "zoRkHxtilXGi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size=128\n",
        "epochs=20"
      ],
      "metadata": {
        "id": "Qy-MeebHlbLE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fitting on Training set & Validating on Validation Set"
      ],
      "metadata": {
        "id": "lByS7AHplc_T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "History = nn_model.fit([train.user_id,train.item_id],train.rating, batch_size=batch_size,\n",
        "                              epochs =epochs, validation_data = ([valid.user_id,valid.item_id],valid.rating),\n",
        "                              verbose = 1)"
      ],
      "metadata": {
        "id": "VAVhfdwAlha9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rcParams['figure.figsize'] = 10, 5\n",
        "\n",
        "plt.plot(History.history['loss'] , 'g')\n",
        "plt.plot(History.history['val_loss'] , 'b')\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "#slight overfitting"
      ],
      "metadata": {
        "id": "_SZqolgxl1Qi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LightGCN (BPR)"
      ],
      "metadata": {
        "id": "GIraTCFboK9X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preparation"
      ],
      "metadata": {
        "id": "6nadnVUwySXU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = df[df['rating']>=3]\n",
        "print(len(df))"
      ],
      "metadata": {
        "id": "IsbiDKsloNNZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Rating Distribution\")\n",
        "df.groupby(['rating'])['rating'].count()"
      ],
      "metadata": {
        "id": "Eu9txpXuunDg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train, test = train_test_split(df.values, test_size=0.2, random_state = 16)\n",
        "train = pd.DataFrame(train, columns = df.columns)\n",
        "test = pd.DataFrame(test, columns = df.columns)"
      ],
      "metadata": {
        "id": "zKGnRSLZuoFI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Train Size  : \", len(train))\n",
        "print(\"Test Size : \", len (test))"
      ],
      "metadata": {
        "id": "yRVbmQ9cyMHt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Label Encoding the User and Item IDs"
      ],
      "metadata": {
        "id": "vBN4KZWvyORZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "le_user = pp.LabelEncoder()\n",
        "le_item = pp.LabelEncoder()\n",
        "train['user_id_idx'] = le_user.fit_transform(train['user_id'].values)\n",
        "train['item_id_idx'] = le_item.fit_transform(train['item_id'].values)"
      ],
      "metadata": {
        "id": "RUG5QALayaYR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_user_ids = train['user_id'].unique()\n",
        "train_item_ids = train['item_id'].unique()\n",
        "\n",
        "print(len(train_user_ids), len(train_item_ids))\n",
        "\n",
        "test = test[(test['user_id'].isin(train_user_ids)) & (test['item_id'].isin(train_item_ids))]\n",
        "print(len(test))"
      ],
      "metadata": {
        "id": "togIsiHlymMC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test['user_id_idx'] = le_user.transform(test['user_id'].values)\n",
        "test['item_id_idx'] = le_item.transform(test['item_id'].values)"
      ],
      "metadata": {
        "id": "C1TBBQAeyoHt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_users = train['user_id_idx'].nunique()\n",
        "n_items = train['item_id_idx'].nunique()\n",
        "print(\"Number of Unique Users : \", n_users)\n",
        "print(\"Number of unique Items : \", n_items)"
      ],
      "metadata": {
        "id": "S0giA7_RywA_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "latent_dim = 64\n",
        "n_layers = 3"
      ],
      "metadata": {
        "id": "OV2pUGbtyxnG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_to_sparse_tensor(dok_mtrx):\n",
        "\n",
        "    dok_mtrx_coo = dok_mtrx.tocoo().astype(np.float32)\n",
        "    values = dok_mtrx_coo.data\n",
        "    indices = np.vstack((dok_mtrx_coo.row, dok_mtrx_coo.col))\n",
        "\n",
        "    i = torch.LongTensor(indices)\n",
        "    v = torch.FloatTensor(values)\n",
        "    shape = dok_mtrx_coo.shape\n",
        "\n",
        "    dok_mtrx_sparse_tensor = torch.sparse.FloatTensor(i, v, torch.Size(shape))\n",
        "\n",
        "    return dok_mtrx_sparse_tensor"
      ],
      "metadata": {
        "id": "l_ZxGvZOyyz5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Modelling"
      ],
      "metadata": {
        "id": "OMHyXopCpmPt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Metrics"
      ],
      "metadata": {
        "id": "x1LEPnRsp4Hh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_metrics(user_Embed_wts, item_Embed_wts, n_users, n_items, train_data, test_data, K):\n",
        "\n",
        "    user_Embedding = nn.Embedding(user_Embed_wts.size()[0], user_Embed_wts.size()[1], _weight = user_Embed_wts)\n",
        "    item_Embedding = nn.Embedding(item_Embed_wts.size()[0], item_Embed_wts.size()[1], _weight = item_Embed_wts)\n",
        "\n",
        "    test_user_ids = torch.LongTensor(test_data['user_id_idx'].unique())\n",
        "\n",
        "    relevance_score = torch.matmul(user_Embed_wts, torch.transpose(item_Embed_wts,0, 1))\n",
        "\n",
        "    R = sp.dok_matrix((n_users, n_items), dtype = np.float32)\n",
        "    R[train_data['user_id_idx'], train_data['item_id_idx']] = 1.0\n",
        "\n",
        "    R_tensor = convert_to_sparse_tensor(R)\n",
        "    R_tensor_dense = R_tensor.to_dense()\n",
        "\n",
        "    R_tensor_dense = R_tensor_dense*(-np.inf)\n",
        "    R_tensor_dense = torch.nan_to_num(R_tensor_dense, nan=0.0)\n",
        "\n",
        "    relevance_score = relevance_score+R_tensor_dense\n",
        "\n",
        "    topk_relevance_score = torch.topk(relevance_score, K).values\n",
        "    topk_relevance_indices = torch.topk(relevance_score, K).indices\n",
        "\n",
        "    topk_relevance_indices_df = pd.DataFrame(topk_relevance_indices.numpy(),columns =['top_indx_'+str(x+1) for x in range(K)])\n",
        "\n",
        "    topk_relevance_indices_df['user_ID'] = topk_relevance_indices_df.index\n",
        "\n",
        "    topk_relevance_indices_df['top_rlvnt_itm'] = topk_relevance_indices_df[['top_indx_'+str(x+1) for x in range(K)]].values.tolist()\n",
        "    topk_relevance_indices_df = topk_relevance_indices_df[['user_ID','top_rlvnt_itm']]\n",
        "\n",
        "    test_interacted_items = test_data.groupby('user_id_idx')['item_id_idx'].apply(list).reset_index()\n",
        "\n",
        "    metrics_df = pd.merge(test_interacted_items,topk_relevance_indices_df, how= 'left', left_on = 'user_id_idx',right_on = ['user_ID'])\n",
        "    metrics_df['intrsctn_itm'] = [list(set(a).intersection(b)) for a, b in zip(metrics_df.item_id_idx, metrics_df.top_rlvnt_itm)]\n",
        "\n",
        "\n",
        "    metrics_df['recall'] = metrics_df.apply(lambda x : len(x['intrsctn_itm'])/len(x['item_id_idx']), axis = 1)\n",
        "    metrics_df['precision'] = metrics_df.apply(lambda x : len(x['intrsctn_itm'])/K, axis = 1)\n",
        "\n",
        "    def get_hit_list(item_id_idx, top_rlvnt_itm):\n",
        "        return [1 if x in set(item_id_idx) else 0 for x in top_rlvnt_itm ]\n",
        "\n",
        "    metrics_df['hit_list'] = metrics_df.apply(lambda x : get_hit_list(x['item_id_idx'], x['top_rlvnt_itm']), axis = 1)\n",
        "\n",
        "    def get_dcg_idcg(item_id_idx, hit_list):\n",
        "        idcg  = sum([1 / np.log1p(idx+1) for idx in range(min(len(item_id_idx),len(hit_list)))])\n",
        "        dcg =  sum([hit / np.log1p(idx+1) for idx, hit in enumerate(hit_list)])\n",
        "        return dcg/idcg\n",
        "\n",
        "    def get_cumsum(hit_list):\n",
        "        return np.cumsum(hit_list)\n",
        "\n",
        "    def get_map(item_id_idx, hit_list, hit_list_cumsum):\n",
        "        return sum([hit_cumsum*hit/(idx+1) for idx, (hit, hit_cumsum) in enumerate(zip(hit_list, hit_list_cumsum))])/len(item_id_idx)\n",
        "\n",
        "    metrics_df['ndcg'] = metrics_df.apply(lambda x : get_dcg_idcg(x['item_id_idx'], x['hit_list']), axis = 1)\n",
        "    metrics_df['hit_list_cumsum'] = metrics_df.apply(lambda x : get_cumsum(x['hit_list']), axis = 1)\n",
        "\n",
        "    metrics_df['map'] = metrics_df.apply(lambda x : get_map(x['item_id_idx'], x['hit_list'], x['hit_list_cumsum']), axis = 1)\n",
        "\n",
        "    return metrics_df['recall'].mean(), metrics_df['precision'].mean(), metrics_df['ndcg'].mean(), metrics_df['map'].mean()"
      ],
      "metadata": {
        "id": "ac5YY42XpnVp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LightGCN Model"
      ],
      "metadata": {
        "id": "G1TWPwsap5oz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LightGCN(nn.Module):\n",
        "    def __init__(self, data, n_users, n_items, n_layers, latent_dim):\n",
        "        super(LightGCN, self).__init__()\n",
        "        self.data = data\n",
        "        self.n_users = n_users\n",
        "        self.n_items = n_items\n",
        "        self.n_layers = n_layers\n",
        "        self.latent_dim = latent_dim\n",
        "        self.init_embedding()\n",
        "        self.norm_adj_mat_sparse_tensor = self.get_A_tilda()\n",
        "\n",
        "    def init_embedding(self):\n",
        "        self.E0 = nn.Embedding(self.n_users + self.n_items, self.latent_dim)\n",
        "        nn.init.xavier_uniform_(self.E0.weight)\n",
        "        self.E0.weight = nn.Parameter(self.E0.weight)\n",
        "\n",
        "    def get_A_tilda(self):\n",
        "        R = sp.dok_matrix((self.n_users, self.n_items), dtype = np.float32)\n",
        "        R[self.data['user_id_idx'], self.data['item_id_idx']] = 1.0\n",
        "\n",
        "        adj_mat = sp.dok_matrix(\n",
        "                (self.n_users + self.n_items, self.n_users + self.n_items), dtype=np.float32\n",
        "            )\n",
        "        adj_mat = adj_mat.tolil()\n",
        "        R = R.tolil()\n",
        "\n",
        "        adj_mat[: n_users, n_users :] = R\n",
        "        adj_mat[n_users :, : n_users] = R.T\n",
        "        adj_mat = adj_mat.todok()\n",
        "\n",
        "        rowsum = np.array(adj_mat.sum(1))\n",
        "        d_inv = np.power(rowsum + 1e-9, -0.5).flatten()\n",
        "        d_inv[np.isinf(d_inv)] = 0.0\n",
        "        d_mat_inv = sp.diags(d_inv)\n",
        "        norm_adj_mat = d_mat_inv.dot(adj_mat)\n",
        "        norm_adj_mat = norm_adj_mat.dot(d_mat_inv)\n",
        "\n",
        "        # Below Code is toconvert the dok_matrix to sparse tensor.\n",
        "\n",
        "        norm_adj_mat_coo = norm_adj_mat.tocoo().astype(np.float32)\n",
        "        values = norm_adj_mat_coo.data\n",
        "        indices = np.vstack((norm_adj_mat_coo.row, norm_adj_mat_coo.col))\n",
        "\n",
        "        i = torch.LongTensor(indices)\n",
        "        v = torch.FloatTensor(values)\n",
        "        shape = norm_adj_mat_coo.shape\n",
        "\n",
        "        norm_adj_mat_sparse_tensor = torch.sparse.FloatTensor(i, v, torch.Size(shape))\n",
        "\n",
        "        return norm_adj_mat_sparse_tensor\n",
        "\n",
        "    def propagate_through_layers(self):\n",
        "        all_layer_embedding = [self.E0.weight]\n",
        "        E_lyr = self.E0.weight\n",
        "\n",
        "        for layer in range(self.n_layers):\n",
        "            E_lyr = torch.sparse.mm(self.norm_adj_mat_sparse_tensor, E_lyr)\n",
        "            all_layer_embedding.append(E_lyr)\n",
        "\n",
        "        all_layer_embedding = torch.stack(all_layer_embedding)\n",
        "        mean_layer_embedding = torch.mean(all_layer_embedding, axis = 0)\n",
        "\n",
        "        final_user_Embed, final_item_Embed = torch.split(mean_layer_embedding, [n_users, n_items])\n",
        "        initial_user_Embed, initial_item_Embed = torch.split(self.E0.weight, [n_users, n_items])\n",
        "\n",
        "        return final_user_Embed, final_item_Embed, initial_user_Embed, initial_item_Embed\n",
        "\n",
        "    def forward(self, users, pos_items, neg_items):\n",
        "        final_user_Embed, final_item_Embed, initial_user_Embed, initial_item_Embed = self.propagate_through_layers()\n",
        "\n",
        "        users_emb, pos_emb, neg_emb = final_user_Embed[users], final_item_Embed[pos_items], final_item_Embed[neg_items]\n",
        "        userEmb0,  posEmb0, negEmb0 = initial_user_Embed[users], initial_item_Embed[pos_items], initial_item_Embed[neg_items]\n",
        "\n",
        "        return users_emb, pos_emb, neg_emb, userEmb0,  posEmb0, negEmb0"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-06-27T20:33:31.37398Z",
          "iopub.execute_input": "2021-06-27T20:33:31.374368Z",
          "iopub.status.idle": "2021-06-27T20:33:31.393303Z",
          "shell.execute_reply.started": "2021-06-27T20:33:31.374336Z",
          "shell.execute_reply": "2021-06-27T20:33:31.392321Z"
        },
        "trusted": true,
        "id": "UPVTHqYEoUwO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lightGCN = LightGCN(train, n_users, n_items, n_layers, latent_dim)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-06-27T20:33:35.180413Z",
          "iopub.execute_input": "2021-06-27T20:33:35.18079Z",
          "iopub.status.idle": "2021-06-27T20:33:35.941784Z",
          "shell.execute_reply.started": "2021-06-27T20:33:35.180754Z",
          "shell.execute_reply": "2021-06-27T20:33:35.940722Z"
        },
        "trusted": true,
        "id": "uFHc3IyOoUwP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Size of Learnable Embedding : \", list(lightGCN.parameters())[0].size())"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-06-27T20:33:38.129087Z",
          "iopub.execute_input": "2021-06-27T20:33:38.129486Z",
          "iopub.status.idle": "2021-06-27T20:33:38.134574Z",
          "shell.execute_reply.started": "2021-06-27T20:33:38.129443Z",
          "shell.execute_reply": "2021-06-27T20:33:38.133797Z"
        },
        "trusted": true,
        "id": "tSWE0b8QoUwQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### BPR Loss"
      ],
      "metadata": {
        "id": "s4fY8Kxkq96m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def bpr_loss(users, users_emb, pos_emb, neg_emb, userEmb0,  posEmb0, negEmb0):\n",
        "\n",
        "    reg_loss = (1/2)*(userEmb0.norm().pow(2) +\n",
        "                    posEmb0.norm().pow(2)  +\n",
        "                    negEmb0.norm().pow(2))/float(len(users))\n",
        "    pos_scores = torch.mul(users_emb, pos_emb)\n",
        "    pos_scores = torch.sum(pos_scores, dim=1)\n",
        "    neg_scores = torch.mul(users_emb, neg_emb)\n",
        "    neg_scores = torch.sum(neg_scores, dim=1)\n",
        "\n",
        "    loss = torch.mean(torch.nn.functional.softplus(neg_scores - pos_scores))\n",
        "\n",
        "    return loss, reg_loss"
      ],
      "metadata": {
        "id": "x5oo3fmHq_z3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def data_loader(data, batch_size, n_usr, n_itm):\n",
        "\n",
        "    interected_items_df = data.groupby('user_id_idx')['item_id_idx'].apply(list).reset_index()\n",
        "\n",
        "    def sample_neg(x):\n",
        "        while True:\n",
        "            neg_id = random.randint(0, n_itm - 1)\n",
        "            if neg_id not in x:\n",
        "                return neg_id\n",
        "\n",
        "    indices = [x for x in range(n_usr)]\n",
        "\n",
        "    if n_usr < batch_size:\n",
        "        users = [random.choice(indices) for _ in range(batch_size)]\n",
        "    else:\n",
        "        users = random.sample(indices, batch_size)\n",
        "\n",
        "    users.sort()\n",
        "\n",
        "    users_df = pd.DataFrame(users,columns = ['users'])\n",
        "\n",
        "    interected_items_df = pd.merge(interected_items_df, users_df, how = 'right', left_on = 'user_id_idx', right_on = 'users')\n",
        "\n",
        "    pos_items = interected_items_df['item_id_idx'].apply(lambda x : random.choice(x)).values\n",
        "\n",
        "    neg_items = interected_items_df['item_id_idx'].apply(lambda x: sample_neg(x)).values\n",
        "\n",
        "    return list(users), list(pos_items), list(neg_items)"
      ],
      "metadata": {
        "id": "OnZyCSFZrDzv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.Adam(lightGCN.parameters(), lr = 0.005)"
      ],
      "metadata": {
        "id": "DUD3T81UrEl8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 30\n",
        "BATCH_SIZE = 1024\n",
        "DECAY = 0.0001\n",
        "K = 10"
      ],
      "metadata": {
        "id": "ls8IlUmYrGKz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training Loop"
      ],
      "metadata": {
        "id": "nORczNeurIST"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss_list_epoch = []\n",
        "MF_loss_list_epoch = []\n",
        "reg_loss_list_epoch = []\n",
        "\n",
        "recall_list = []\n",
        "precision_list = []\n",
        "ndcg_list = []\n",
        "map_list = []\n",
        "\n",
        "train_time_list = []\n",
        "eval_time_list = []\n",
        "\n",
        "for epoch in tqdm(range(EPOCHS)):\n",
        "    n_batch = int(len(train)/BATCH_SIZE)\n",
        "\n",
        "    final_loss_list = []\n",
        "    MF_loss_list = []\n",
        "    reg_loss_list = []\n",
        "\n",
        "    best_ndcg = -1\n",
        "\n",
        "    train_start_time = time.time()\n",
        "    lightGCN.train()\n",
        "    for batch_idx in range(n_batch):\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        users, pos_items, neg_items = data_loader(train, BATCH_SIZE, n_users, n_items)\n",
        "\n",
        "        users_emb, pos_emb, neg_emb, userEmb0,  posEmb0, negEmb0 = lightGCN.forward(users, pos_items, neg_items)\n",
        "\n",
        "        mf_loss, reg_loss = bpr_loss(users, users_emb, pos_emb, neg_emb, userEmb0,  posEmb0, negEmb0)\n",
        "        reg_loss = DECAY * reg_loss\n",
        "        final_loss = mf_loss + reg_loss\n",
        "\n",
        "        final_loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        final_loss_list.append(final_loss.item())\n",
        "        MF_loss_list.append(mf_loss.item())\n",
        "        reg_loss_list.append(reg_loss.item())\n",
        "\n",
        "\n",
        "    train_end_time = time.time()\n",
        "    train_time = train_end_time - train_start_time\n",
        "\n",
        "    lightGCN.eval()\n",
        "    with torch.no_grad():\n",
        "\n",
        "        final_user_Embed, final_item_Embed, initial_user_Embed,initial_item_Embed = lightGCN.propagate_through_layers()\n",
        "        test_topK_recall,  test_topK_precision, test_topK_ndcg, test_topK_map  = get_metrics(final_user_Embed, final_item_Embed, n_users, n_items, train, test, K)\n",
        "\n",
        "\n",
        "    if test_topK_ndcg > best_ndcg:\n",
        "        best_ndcg = test_topK_ndcg\n",
        "\n",
        "        torch.save(final_user_Embed, 'final_user_Embed.pt')\n",
        "        torch.save(final_item_Embed, 'final_item_Embed.pt')\n",
        "        torch.save(initial_user_Embed, 'initial_user_Embed.pt')\n",
        "        torch.save(initial_item_Embed, 'initial_item_Embed.pt')\n",
        "\n",
        "\n",
        "    eval_time = time.time() - train_end_time\n",
        "\n",
        "    loss_list_epoch.append(round(np.mean(final_loss_list),4))\n",
        "    MF_loss_list_epoch.append(round(np.mean(MF_loss_list),4))\n",
        "    reg_loss_list_epoch.append(round(np.mean(reg_loss_list),4))\n",
        "\n",
        "    recall_list.append(round(test_topK_recall,4))\n",
        "    precision_list.append(round(test_topK_precision,4))\n",
        "    ndcg_list.append(round(test_topK_ndcg,4))\n",
        "    map_list.append(round(test_topK_map,4))\n",
        "\n",
        "    train_time_list.append(train_time)\n",
        "    eval_time_list.append(eval_time)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-06-27T20:34:12.936367Z",
          "iopub.execute_input": "2021-06-27T20:34:12.936717Z",
          "iopub.status.idle": "2021-06-27T20:38:48.98772Z",
          "shell.execute_reply.started": "2021-06-27T20:34:12.936689Z",
          "shell.execute_reply": "2021-06-27T20:38:48.986382Z"
        },
        "trusted": true,
        "id": "iV_pWNwDoUwY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epoch_list = [(i+1) for i in range(EPOCHS)]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-06-27T20:39:10.394779Z",
          "iopub.execute_input": "2021-06-27T20:39:10.395209Z",
          "iopub.status.idle": "2021-06-27T20:39:10.400446Z",
          "shell.execute_reply.started": "2021-06-27T20:39:10.395175Z",
          "shell.execute_reply": "2021-06-27T20:39:10.399437Z"
        },
        "trusted": true,
        "id": "aQcrD4GIoUwZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(epoch_list, recall_list, label='Recall')\n",
        "plt.plot(epoch_list, precision_list, label='Precision')\n",
        "plt.plot(epoch_list, ndcg_list, label='NDCG')\n",
        "plt.plot(epoch_list, map_list, label='MAP')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Metrics')\n",
        "plt.legend()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-06-27T20:39:12.438542Z",
          "iopub.execute_input": "2021-06-27T20:39:12.438887Z",
          "iopub.status.idle": "2021-06-27T20:39:12.715678Z",
          "shell.execute_reply.started": "2021-06-27T20:39:12.438858Z",
          "shell.execute_reply": "2021-06-27T20:39:12.714486Z"
        },
        "trusted": true,
        "id": "8MCkPCm_oUwZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(epoch_list, loss_list_epoch, label='Total Training Loss')\n",
        "plt.plot(epoch_list, MF_loss_list_epoch, label='MF Training Loss')\n",
        "plt.plot(epoch_list, reg_loss_list_epoch, label='Reg Training Loss')\n",
        "\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-06-27T20:39:26.415329Z",
          "iopub.execute_input": "2021-06-27T20:39:26.415926Z",
          "iopub.status.idle": "2021-06-27T20:39:26.641685Z",
          "shell.execute_reply.started": "2021-06-27T20:39:26.415884Z",
          "shell.execute_reply": "2021-06-27T20:39:26.640497Z"
        },
        "trusted": true,
        "id": "4OGvlt2uoUwb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Averge time taken to train an epoch -> \", round(np.mean(train_time_list),2), \" seconds\")\n",
        "print(\"Averge time taken to eval an epoch -> \", round(np.mean(eval_time_list),2), \" seconds\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-06-27T20:39:35.77767Z",
          "iopub.execute_input": "2021-06-27T20:39:35.778091Z",
          "iopub.status.idle": "2021-06-27T20:39:35.785068Z",
          "shell.execute_reply.started": "2021-06-27T20:39:35.778051Z",
          "shell.execute_reply": "2021-06-27T20:39:35.784097Z"
        },
        "trusted": true,
        "id": "dudiPvfgoUwb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Last Epoch's Test Data Recall -> \", recall_list[-1])\n",
        "print(\"Last Epoch's Test Data Precision -> \", precision_list[-1])\n",
        "print(\"Last Epoch's Test Data NDCG -> \", ndcg_list[-1])\n",
        "print(\"Last Epoch's Test Data MAP -> \", map_list[-1])\n",
        "\n",
        "print(\"Last Epoch's Train Data Loss -> \", loss_list_epoch[-1])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-06-27T20:40:11.483313Z",
          "iopub.execute_input": "2021-06-27T20:40:11.483701Z",
          "iopub.status.idle": "2021-06-27T20:40:11.491432Z",
          "shell.execute_reply.started": "2021-06-27T20:40:11.483668Z",
          "shell.execute_reply": "2021-06-27T20:40:11.489958Z"
        },
        "trusted": true,
        "id": "xCoH67EAoUwb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}