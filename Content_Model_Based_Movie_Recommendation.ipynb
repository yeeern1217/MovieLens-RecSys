{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JbdngmepFgVw"
      },
      "outputs": [],
      "source": [
        "!pip install torch_geometric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6MLX4hX8f8bn"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV, StratifiedKFold\n",
        "from sklearn import preprocessing as pp\n",
        "from sklearn.metrics import make_scorer\n",
        "from torch_geometric.data import HeteroData, download_url, extract_zip\n",
        "from keras.layers import Dropout, Flatten, Activation, Input, Embedding, BatchNormalization, Dense, dot\n",
        "from keras.optimizers import Adam\n",
        "from pylab import rcParams\n",
        "from tqdm.notebook import tqdm\n",
        "from sklearn.metrics import roc_auc_score, accuracy_score\n",
        "from scipy.stats import randint, uniform\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import keras\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import scipy.sparse as sp\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import random\n",
        "import xgboost as xgb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CAdr4osSFLnj"
      },
      "outputs": [],
      "source": [
        "def download(url, root=os.getcwd()) -> None:\n",
        "    # ref: https://pytorch-geometric.readthedocs.io/en/stable/_modules/torch_geometric/datasets/movie_lens_100k.html#MovieLens100K\n",
        "    path = download_url(url, root)\n",
        "    extract_zip(path, root)\n",
        "    os.remove(path)\n",
        "\n",
        "    folder_name = url.split(\"/\")[-1].split(\".\")[0]\n",
        "    # folder = os.path.join(root, folder_name)\n",
        "    # fs.rm(raw_dir)\n",
        "    # os.rename(folder, raw_dir)\n",
        "    return os.path.join(root, folder_name)\n",
        "\n",
        "url = \"https://files.grouplens.org/datasets/movielens/ml-100k.zip\"\n",
        "\n",
        "raw_file_names = [\n",
        "    \"u.item\",\n",
        "    \"u.user\",\n",
        "    \"u.data\",\n",
        "]\n",
        "# ['u.item', 'u.user', 'u1.base', 'u1.test']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VY6qyyMeE7WG"
      },
      "outputs": [],
      "source": [
        "USER_HEADERS = [\"user_id\", \"age\", \"gender\", \"occupation\", \"zip_code\"]\n",
        "MOVIE_HEADERS = [\n",
        "    \"item_id\",\n",
        "    \"title\",\n",
        "    \"release_date\",\n",
        "    \"video_release_date\",\n",
        "    \"IMDb URL\",\n",
        "    \"unknown\",\n",
        "    \"Action\",\n",
        "    \"Adventure\",\n",
        "    \"Animation\",\n",
        "    \"Children's\",\n",
        "    \"Comedy\",\n",
        "    \"Crime\",\n",
        "    \"Documentary\",\n",
        "    \"Drama\",\n",
        "    \"Fantasy\",\n",
        "    \"Film-Noir\",\n",
        "    \"Horror\",\n",
        "    \"Musical\",\n",
        "    \"Mystery\",\n",
        "    \"Romance\",\n",
        "    \"Sci-Fi\",\n",
        "    \"Thriller\",\n",
        "    \"War\",\n",
        "    \"Western\",\n",
        "]\n",
        "RATING_HEADERS = [\"user_id\", \"item_id\", \"rating\", \"timestamp\"]\n",
        "\n",
        "\n",
        "folder_path = download(url)\n",
        "raw_paths = [os.path.join(folder_path, i) for i in raw_file_names]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2CjsFd_laV5P"
      },
      "outputs": [],
      "source": [
        "user_df = pd.read_csv(\n",
        "    raw_paths[1],\n",
        "    sep=\"|\",\n",
        "    header=None,\n",
        "    names=USER_HEADERS,\n",
        "    # index_col='user_id',\n",
        "    encoding=\"ISO-8859-1\",\n",
        ")\n",
        "\n",
        "item_df = pd.read_csv(\n",
        "    raw_paths[0],\n",
        "    sep=\"|\",\n",
        "    header=None,\n",
        "    names=MOVIE_HEADERS,\n",
        "    # index_col='item_id',\n",
        "    encoding=\"ISO-8859-1\",\n",
        ")\n",
        "\n",
        "rating_df = pd.read_csv(\n",
        "    raw_paths[2],\n",
        "    sep=\"\\t\",\n",
        "    header=None,\n",
        "    names=RATING_HEADERS,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "rating_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qC3DdQj6Y0Na"
      },
      "outputs": [],
      "source": [
        "user_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-bfyGC8XY1ri"
      },
      "outputs": [],
      "source": [
        "item_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FiJVSZjNdvsI"
      },
      "source": [
        "## Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nt9T9AsB5CKL"
      },
      "outputs": [],
      "source": [
        "# Consider ratings of 4 or higher as a positive interaction.\n",
        "positive_ratings = rating_df[rating_df['rating'] >= 4]\n",
        "positive_ratings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get the genre columns from the item_df\n",
        "genre_cols = item_df.columns[item_df.columns.str.startswith(('Action', 'Adventure', 'Animation', \"Children's\", 'Comedy', 'Crime', 'Documentary', 'Drama', 'Fantasy', 'Film-Noir', 'Horror', 'Musical', 'Mystery', 'Romance', 'Sci-Fi', 'Thriller', 'War', 'Western'))]\n",
        "genre_cols"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gKIiYK005nIY"
      },
      "outputs": [],
      "source": [
        "# Merge positive ratings with movie genres\n",
        "merged_df = pd.merge(positive_ratings, item_df, left_on='item_id', right_on='item_id')\n",
        "merged_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create user profiles by averaging the genres of movies they liked\n",
        "user_profiles = merged_df.groupby('user_id')[genre_cols].mean()\n",
        "user_profiles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "neg_sample_ratio = 5\n",
        "all_item_ids = item_df['item_id'].unique()\n",
        "\n",
        "print(f\"Negative sample ratio set to: {neg_sample_ratio}\")\n",
        "print(f\"Total unique movie IDs available: {len(all_item_ids)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "rated_movies_by_user = rating_df.groupby('user_id')['item_id'].apply(set)\n",
        "\n",
        "print(\"Sample for User 1:\", rated_movies_by_user.get(1))\n",
        "print(\"Sample for User 2:\", rated_movies_by_user.get(2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "training_data = []\n",
        "\n",
        "for _, row in tqdm(positive_ratings.iterrows(), total=positive_ratings.shape[0]):\n",
        "    user_id = int(row['user_id'])\n",
        "    item_id = int(row['item_id'])\n",
        "    timestamp = int(row['timestamp']) # Get the original timestamp\n",
        "\n",
        "    # Add positive sample\n",
        "    positive_entry = {'user_id': user_id, 'item_id': item_id, 'target': 1, 'timestamp': timestamp}\n",
        "    training_data.append(positive_entry)\n",
        "\n",
        "    # Add negative samples\n",
        "    current_rated_movies = rated_movies_by_user.get(user_id, set())\n",
        "    for neg_idx in range(neg_sample_ratio):\n",
        "        while True:\n",
        "            # Randomly pick a movie ID\n",
        "            random_item_id = random.choice(all_item_ids)\n",
        "            # Check if it's a true negative (user hasn't rated it)\n",
        "            if random_item_id not in current_rated_movies:\n",
        "                negative_entry = {'user_id': user_id, 'item_id': random_item_id, 'target': 0, 'timestamp': timestamp}\n",
        "                training_data.append(negative_entry)\n",
        "                break\n",
        "            else:\n",
        "                pass # Keep picking until a true negative is found"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "training_df = pd.DataFrame(training_data)\n",
        "\n",
        "print(f\"Training set created with {len(training_df)} samples.\")\n",
        "print(\"Sample of the training set:\")\n",
        "print(training_df.head(10)) # Show more rows to see both positive and negative\n",
        "print(\"\\nValue counts for 'target' column:\")\n",
        "print(training_df['target'].value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qBG_kKDMRSBR"
      },
      "outputs": [],
      "source": [
        "# Merge all features into the training DataFrame\n",
        "\n",
        "# Merge user profiles (user's taste)\n",
        "training_df = pd.merge(training_df, user_profiles, on='user_id', how='left')\n",
        "# Rename user profile genres to distinguish them from movie genres\n",
        "training_df.rename(columns={g: f'user_{g}' for g in genre_cols}, inplace=True)\n",
        "\n",
        "# Merge item (movie) features using the index of item_df\n",
        "training_df = pd.merge(training_df, item_df[genre_cols], left_on='item_id', right_index=True, how='left')\n",
        "\n",
        "# Fill any potential NaNs (for users who might not have a profile yet)\n",
        "training_df.fillna(0, inplace=True)\n",
        "\n",
        "training_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29_e5BpZVNn2"
      },
      "source": [
        "### Modelling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DBxOJla2RlWX"
      },
      "outputs": [],
      "source": [
        "features = [col for col in training_df.columns if col not in ['user_id', 'item_id', 'target', 'timestamp']]\n",
        "# Sort by user and then by timestamp\n",
        "training_df.sort_values(by=['user_id', 'timestamp'], inplace=True)\n",
        "\n",
        "# Defining test ratio \n",
        "test_ratio_per_user = 0.2\n",
        "\n",
        "# Initialize lists to store indices for train and test sets\n",
        "train_indices = []\n",
        "test_indices = []\n",
        "\n",
        "# Group by user and split\n",
        "for user_id, group in tqdm(training_df.groupby('user_id'), desc=\"Splitting per user\"):\n",
        "    num_samples = len(group)\n",
        "    split_point = int(num_samples * (1 - test_ratio_per_user))\n",
        "\n",
        "    # Get indices for train and test based on the sorted group\n",
        "    train_indices.extend(group.iloc[:split_point].index.tolist())\n",
        "    test_indices.extend(group.iloc[split_point:].index.tolist())\n",
        "\n",
        "# Create X_train, X_test, y_train, y_test using the collected indices\n",
        "X_train = training_df.loc[train_indices, features]\n",
        "y_train = training_df.loc[train_indices, 'target']\n",
        "\n",
        "X_test = training_df.loc[test_indices, features]\n",
        "y_test = training_df.loc[test_indices, 'target']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize and train the XGBoost classifier\n",
        "xgb_model = xgb.XGBClassifier(\n",
        "    objective='binary:logistic',\n",
        "    eval_metric='logloss',\n",
        "    n_estimators=200,\n",
        "    learning_rate=0.1,\n",
        "    max_depth=4,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    random_state=42,\n",
        "    use_label_encoder=False # Suppress a warning\n",
        ")\n",
        "\n",
        "\n",
        "xgb_model.fit(X_train, y_train)\n",
        "y_pred_proba = xgb_model.predict_proba(X_test)[:, 1]\n",
        "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
        "\n",
        "print(f\"ROC AUC Score: {roc_auc:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ranking-based evaluation metrics\n",
        "def precision_at_k(y_true, y_scores, k):\n",
        "    \"\"\"Calculate precision@k\"\"\"\n",
        "    # Get top k predictions\n",
        "    top_k_idx = np.argsort(y_scores)[::-1][:k]\n",
        "    top_k_true = y_true.iloc[top_k_idx] if hasattr(y_true, 'iloc') else y_true[top_k_idx]\n",
        "    return np.sum(top_k_true) / k\n",
        "\n",
        "def recall_at_k(y_true, y_scores, k):\n",
        "    \"\"\"Calculate recall@k\"\"\"\n",
        "    # Get top k predictions\n",
        "    top_k_idx = np.argsort(y_scores)[::-1][:k]\n",
        "    top_k_true = y_true.iloc[top_k_idx] if hasattr(y_true, 'iloc') else y_true[top_k_idx]\n",
        "    total_relevant = np.sum(y_true)\n",
        "    if total_relevant == 0:\n",
        "        return 0.0\n",
        "    return np.sum(top_k_true) / total_relevant\n",
        "\n",
        "def average_precision_at_k(y_true, y_scores, k):\n",
        "    \"\"\"Calculate average precision@k (AP@k)\"\"\"\n",
        "    # Get top k predictions\n",
        "    top_k_idx = np.argsort(y_scores)[::-1][:k]\n",
        "    top_k_true = y_true.iloc[top_k_idx] if hasattr(y_true, 'iloc') else y_true[top_k_idx]\n",
        "    \n",
        "    if np.sum(top_k_true) == 0:\n",
        "        return 0.0\n",
        "    \n",
        "    # Calculate precision at each position where there's a relevant item\n",
        "    precisions = []\n",
        "    for i in range(k):\n",
        "        if top_k_true[i] == 1:  # If item at position i is relevant\n",
        "            precision_at_i = np.sum(top_k_true[:i+1]) / (i + 1)\n",
        "            precisions.append(precision_at_i)\n",
        "    \n",
        "    if len(precisions) == 0:\n",
        "        return 0.0\n",
        "    \n",
        "    return np.mean(precisions)\n",
        "\n",
        "def mean_average_precision_at_k(y_true_per_user, y_scores_per_user, k):\n",
        "    \"\"\"Calculate mean average precision@k (mAP@k) across all users\"\"\"\n",
        "    ap_scores = []\n",
        "    for y_true, y_scores in zip(y_true_per_user, y_scores_per_user):\n",
        "        if len(y_true) > 0:  # Only calculate if user has test data\n",
        "            ap = average_precision_at_k(y_true, y_scores, min(k, len(y_true)))\n",
        "            ap_scores.append(ap)\n",
        "    \n",
        "    return np.mean(ap_scores) if ap_scores else 0.0\n",
        "\n",
        "def evaluate_ranking_metrics(model, X_test, y_test, test_indices, training_df, k_values=[5, 10, 20]):\n",
        "    \"\"\"Evaluate precision@k, recall@k, and mAP@k for each user\"\"\"\n",
        "    \n",
        "    # Get predictions\n",
        "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
        "    \n",
        "    # Create a test dataframe with predictions\n",
        "    test_df = training_df.loc[test_indices].copy()\n",
        "    test_df['prediction_score'] = y_pred_proba\n",
        "    \n",
        "    results = {}\n",
        "    \n",
        "    for k in k_values:\n",
        "        precision_scores = []\n",
        "        recall_scores = []\n",
        "        ap_scores = []\n",
        "        \n",
        "        # Calculate metrics per user\n",
        "        for user_id, user_group in test_df.groupby('user_id'):\n",
        "            y_true_user = user_group['target'].values\n",
        "            y_scores_user = user_group['prediction_score'].values\n",
        "            \n",
        "            # Only evaluate if user has enough test samples\n",
        "            if len(y_true_user) >= k:\n",
        "                prec_k = precision_at_k(y_true_user, y_scores_user, k)\n",
        "                rec_k = recall_at_k(y_true_user, y_scores_user, k)\n",
        "                ap_k = average_precision_at_k(y_true_user, y_scores_user, k)\n",
        "                \n",
        "                precision_scores.append(prec_k)\n",
        "                recall_scores.append(rec_k)\n",
        "                ap_scores.append(ap_k)\n",
        "        \n",
        "        # Calculate mean metrics\n",
        "        mean_precision = np.mean(precision_scores) if precision_scores else 0.0\n",
        "        mean_recall = np.mean(recall_scores) if recall_scores else 0.0\n",
        "        mean_ap = np.mean(ap_scores) if ap_scores else 0.0\n",
        "        \n",
        "        results[k] = {\n",
        "            'precision': mean_precision,\n",
        "            'recall': mean_recall,\n",
        "            'mAP': mean_ap,\n",
        "            'num_users_evaluated': len(precision_scores)\n",
        "        }\n",
        "    \n",
        "    return results\n",
        "\n",
        "k_values = [5, 10, 20]\n",
        "ranking_results = evaluate_ranking_metrics(xgb_model, X_test, y_test, test_indices, training_df, k_values)\n",
        "\n",
        "for k in k_values:\n",
        "    results = ranking_results[k]\n",
        "    print(f\"Results for K={k}:\")\n",
        "    print(f\"   Precision@{k}: {results['precision']:.4f}\")\n",
        "    print(f\"   Recall@{k}:    {results['recall']:.4f}\")\n",
        "    print(f\"   mAP@{k}:       {results['mAP']:.4f}\")\n",
        "    print(f\"   Users evaluated: {results['num_users_evaluated']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vlIdQcLRRnAg"
      },
      "outputs": [],
      "source": [
        "def get_recommendations(user_id, model, user_profiles, item_df, rating_df, top_n=10):\n",
        "    # Get movies the user has already rated\n",
        "    rated_item_ids = rating_df[rating_df['user_id'] == user_id]['item_id'].unique()\n",
        "\n",
        "    # Create a DataFrame of candidate movies (all movies not yet rated)\n",
        "    candidate_movies = item_df[~item_df['item_id'].isin(rated_item_ids)].copy()\n",
        "    candidate_movies['user_id'] = user_id\n",
        "\n",
        "    # Create a copy of user profiles to avoid modifying the original DataFrame.\n",
        "    user_profile_to_merge = user_profiles.copy()\n",
        "\n",
        "    # Rename the columns to match the feature names used in training (e.g., 'Action' -> 'user_Action').\n",
        "    user_profile_to_merge.columns = [f'user_{col}' for col in user_profile_to_merge.columns]\n",
        "\n",
        "    # Merge the prepared user profile data. Since user_profiles is indexed by user_id,\n",
        "    #    we merge on the index. This avoids column name collisions.\n",
        "    candidate_movies = pd.merge(candidate_movies, user_profile_to_merge, left_on='user_id', right_index=True, how='left')\n",
        "\n",
        "    # Fill any NaNs that might result from the merge (e.g., a user with no positive ratings).\n",
        "    candidate_movies.fillna(0, inplace=True)\n",
        "    candidate_features = candidate_movies[features]\n",
        "\n",
        "    # Predict the probability of liking each candidate movie\n",
        "    candidate_movies['recommendation_score'] = model.predict_proba(candidate_features)[:, 1]\n",
        "\n",
        "    # Sort by score and return the top N\n",
        "    recommendations = candidate_movies.sort_values('recommendation_score', ascending=False).head(top_n)\n",
        "\n",
        "    return recommendations[['item_id', 'title', 'recommendation_score']]\n",
        "\n",
        "sample_user_id = 50\n",
        "print(f\"\\Top 10 Recommendations for User ID {sample_user_id}:\")\n",
        "recommendations = get_recommendations(sample_user_id, xgb_model, user_profiles, item_df, rating_df)\n",
        "print(recommendations)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
